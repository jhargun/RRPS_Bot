{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate performance against all bots\n",
        "\n",
        "To see the result of running this code, see `results.txt`\n",
        "\n",
        "This notebook contains code for evaluating my agent. Note that you must ensure `model_stateDict.pt` is present. Also note that this notebook saves its results to Google Drive. This is because I was running it on Colab and wanted to avoid losing the data when my session was ended. You can either run this notebook on Colab or remove the Google Drive portion if you want to run locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wb85AdOFdH_H",
        "outputId": "0379f44e-f4e5-4e83-c7cf-9e7746b6f827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: open_spiel in /usr/local/lib/python3.10/dist-packages (1.4)\n",
            "Requirement already satisfied: pip>=20.0.2 in /usr/local/lib/python3.10/dist-packages (from open_spiel) (23.1.2)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.10/dist-packages (from open_spiel) (23.1.0)\n",
            "Requirement already satisfied: absl-py>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from open_spiel) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.21.5 in /usr/local/lib/python3.10/dist-packages (from open_spiel) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from open_spiel) (1.11.4)\n",
            "Requirement already satisfied: ml-collections>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from open_spiel) (0.1.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from ml-collections>=0.1.1->open_spiel) (6.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml-collections>=0.1.1->open_spiel) (1.16.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml-collections>=0.1.1->open_spiel) (21.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade open_spiel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5hCiqDHkdLcX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from open_spiel.python import rl_agent\n",
        "from open_spiel.python import rl_environment\n",
        "import pyspiel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDiM8eY_dW2Q",
        "outputId": "be2f609f-d344-42b8-dbc8-ae1a4df2b300"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading bot population...\n",
            "Population size: 43\n"
          ]
        }
      ],
      "source": [
        "RECALL = 20\n",
        "\n",
        "# The population of 43 bots. See the RRPS paper for high-level descriptions of\n",
        "# what each bot does.\n",
        "\n",
        "print(\"Loading bot population...\")\n",
        "pop_size = pyspiel.ROSHAMBO_NUM_BOTS\n",
        "print(f\"Population size: {pop_size}\")\n",
        "roshambo_bot_names = pyspiel.roshambo_bot_names()\n",
        "roshambo_bot_names.sort()\n",
        "\n",
        "bot_id = 0\n",
        "roshambo_bot_ids = {}\n",
        "for name in roshambo_bot_names:\n",
        "  roshambo_bot_ids[name] = bot_id\n",
        "  bot_id += 1\n",
        "\n",
        "roshambo_id_to_name = {v: k for k, v in roshambo_bot_ids.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remove the following cell if you want to run locally. If you do this, make sure to update the save path later in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fVkbuUvdvnH",
        "outputId": "0afa1ad4-4046-4473-d813-25cce51fb723"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following cell will print a warning if not on a machine with a GPU. Feel free to ignore this if running on a CPU. The model is small enough that you can run on a CPU (this is what I did when evaluating the model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xch1HYa_dypG",
        "outputId": "af804293-7237-4207-8929-a020af3cf92a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: torch did not find GPU\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "gpuAvailable = torch.cuda.is_available()\n",
        "if not gpuAvailable:\n",
        "  print(\"Warning: torch did not find GPU\")\n",
        "\n",
        "WINDOW_SIZE = 200   # Size of the window to use for the LSTM\n",
        "NUM_WINDOWS_PER_SERIES = 1000 - WINDOW_SIZE  # Number of windows per time series\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    This model predicts the opponent's next move given 2 inputs:\n",
        "    1. The first WINDOW_SIZE moves of the opponent and agent (agent acting purely randomly)\n",
        "    2. The previous WINDOW_SIZE moves of the opponent and agent\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        lstmHiddenSize = 100\n",
        "        self.lstm = nn.LSTM(input_size=6, hidden_size=lstmHiddenSize, batch_first=True)\n",
        "        if gpuAvailable:\n",
        "          self.lstm.cuda()\n",
        "\n",
        "        startingFirstActionSize = WINDOW_SIZE * 6\n",
        "        self.firstResponsesNetwork = nn.Sequential(\n",
        "            nn.Linear(startingFirstActionSize, startingFirstActionSize // 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(startingFirstActionSize // 2, startingFirstActionSize // 4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(startingFirstActionSize // 4, startingFirstActionSize // 6),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        if gpuAvailable:\n",
        "          self.firstResponsesNetwork.cuda()\n",
        "\n",
        "        startingCombinedSize = lstmHiddenSize + startingFirstActionSize // 6\n",
        "        self.combinedNetwork = nn.Sequential(\n",
        "            nn.Linear(startingCombinedSize, startingCombinedSize // 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(startingCombinedSize // 2, startingCombinedSize // 4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(startingCombinedSize // 4, 3),\n",
        "            nn.Softmax(dim=1),\n",
        "        )\n",
        "        if gpuAvailable:\n",
        "          self.combinedNetwork.cuda()\n",
        "\n",
        "    def forward(self, recentActions, firstActions):\n",
        "        x1, _ = self.lstm(recentActions)\n",
        "        x1 = x1[:, -1, :]  # Get only the last output\n",
        "        x2 = self.firstResponsesNetwork(firstActions)\n",
        "        x = torch.cat((x1.view(x1.size(0), -1), x2.view(x2.size(0), -1)), dim=1)\n",
        "        x = self.combinedNetwork(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Y5DUWEL2egwk"
      },
      "outputs": [],
      "source": [
        "from random import random, randint\n",
        "\n",
        "class LstmAgent(rl_agent.AbstractAgent):\n",
        "  def __init__(self, model_path: str = 'model_stateDict.pt', num_actions: int = 3, name: str = \"lstm_agent\", random_chance: float = .8):\n",
        "    assert num_actions > 0\n",
        "    self._num_actions = num_actions  # 3\n",
        "    assert 0 <= random_chance <= 1, \"Random chance must be between 0 and 1\"\n",
        "    self.random_chance = random_chance\n",
        "    self.stepNum = 0\n",
        "\n",
        "    self.model = LSTM()\n",
        "    if gpuAvailable:  # Model was trained on GPU, so need to map if not inferencing on GPU\n",
        "      self.model.load_state_dict(torch.load(model_path))\n",
        "    else:\n",
        "      self.model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "    self.model.eval()\n",
        "\n",
        "  def restart(self):\n",
        "    self.stepNum = 0\n",
        "\n",
        "  def convertHistoryToOneHotEncoding(self, history: list[int]) -> torch.Tensor:\n",
        "    assert len(history) == 2 * WINDOW_SIZE, f\"History has {len(history)} elements (expected {2 * WINDOW_SIZE})\"\n",
        "    result = np.array(history).reshape((len(history) // 2, 2))  # Reshape results\n",
        "    result = np.flip(result, axis=1).copy()  # Have opponent predictions first to match model's expected format\n",
        "    result = F.one_hot(torch.tensor(result), 3)   # Convert to one-hot\n",
        "    result = torch.reshape(result, (1, len(result),6)) # Reshape again (into format expected by model)\n",
        "    return result.type(torch.float32)\n",
        "\n",
        "  def step(self, time_step, is_evaluation=False):\n",
        "    # If it is the end of the episode, don't select an action.\n",
        "    if time_step.last():\n",
        "      return\n",
        "\n",
        "    probs = np.ones(self._num_actions) / self._num_actions\n",
        "\n",
        "    # For the first WINDOW_SIZE steps, return a random action\n",
        "    if self.stepNum < WINDOW_SIZE:\n",
        "      self.stepNum += 1\n",
        "      return rl_agent.StepOutput(action=randint(0,2), probs=probs)\n",
        "\n",
        "    \"\"\"\n",
        "    Choose a random action some amount of the time. This seems to improve performance. My theory is\n",
        "    that this makes the history more similar to what the model was trained on, which is an opponent's\n",
        "    response to completely random inputs. Of course, random chance can't beat most others consistently,\n",
        "    so when choosing the randomness, you want to have enough randomness that the model can perform well\n",
        "    but not so much randomness that the model doesn't get enough chances to beat Greenberg.\n",
        "    If self.random_chance is 0, it will always use the model.\n",
        "    If self.random_chance is 1, it will always be random (equivalent to randbot).\n",
        "    \"\"\"\n",
        "    if self.random_chance > 0 and (self.random_chance == 1 or random() < self.random_chance):\n",
        "      self.stepNum += 1\n",
        "      return rl_agent.StepOutput(action=randint(0,2), probs=probs)\n",
        "    else:\n",
        "      # Run history through the LSTM to predict what opponent will do next\n",
        "      game, state = pyspiel.deserialize_game_and_state(time_step.observations[\"serialized_state\"])\n",
        "      history = state.history()\n",
        "      prev200 = self.convertHistoryToOneHotEncoding(history[-400:])\n",
        "      first200 = self.convertHistoryToOneHotEncoding(history[:400])\n",
        "      first200 = torch.unsqueeze(torch.flatten(first200), 0)\n",
        "\n",
        "      prediction = self.model(prev200, first200)[0].argmax().item()\n",
        "      action = (prediction + 1) % 3  # Select the action that beats what you think opponent will do\n",
        "\n",
        "      return rl_agent.StepOutput(action=action, probs=probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4uIH8U73hQSp"
      },
      "outputs": [],
      "source": [
        "class BotAgent(rl_agent.AbstractAgent):\n",
        "  \"\"\"Agent class that wraps a bot.\n",
        "\n",
        "  Note, the environment must include the OpenSpiel state in its observations,\n",
        "  which means it must have been created with use_full_state=True.\n",
        "\n",
        "  This is a simple wrapper that lets the RPS bots be interpreted as agents under\n",
        "  the RL API.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_actions, bot, name=\"bot_agent\"):\n",
        "    assert num_actions > 0\n",
        "    self._bot = bot\n",
        "    self._num_actions = num_actions\n",
        "\n",
        "  def restart(self):\n",
        "    self._bot.restart()\n",
        "\n",
        "  def step(self, time_step, is_evaluation=False):\n",
        "    # If it is the end of the episode, don't select an action.\n",
        "    if time_step.last():\n",
        "      return\n",
        "    _, state = pyspiel.deserialize_game_and_state(\n",
        "        time_step.observations[\"serialized_state\"])\n",
        "    action = self._bot.step(state)\n",
        "    probs = np.zeros(self._num_actions)\n",
        "    probs[action] = 1.0\n",
        "    return rl_agent.StepOutput(action=action, probs=probs)\n",
        "\n",
        "def create_roshambo_bot_agent(player_id, num_actions, bot_names, pop_id):\n",
        "  name = bot_names[pop_id]\n",
        "  # Creates an OpenSpiel bot with the default number of throws\n",
        "  # (pyspiel.ROSHAMBO_NUM_THROWS). To create one for a different number of\n",
        "  # throws per episode, add the number as the third argument here.\n",
        "  bot = pyspiel.make_roshambo_bot(player_id, name)\n",
        "  return BotAgent(num_actions, bot, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "V15gpmFdeC38"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def eval_agents_count_winrate(env, agents, num_players, num_episodes, verbose=False):\n",
        "  \"\"\"Slightly altered to count number of wins/losses/draws\"\"\"\n",
        "  sum_episode_rewards = np.zeros(num_players)\n",
        "  wins = 0\n",
        "  draws = 0\n",
        "  losses = 0\n",
        "\n",
        "  for ep in tqdm(range(num_episodes)):\n",
        "  # for ep in range(num_episodes):\n",
        "    for agent in agents:\n",
        "      # Bots need to be restarted at the start of the episode.\n",
        "      if hasattr(agent, \"restart\"):\n",
        "        agent.restart()\n",
        "    time_step = env.reset()\n",
        "    episode_rewards = np.zeros(num_players)\n",
        "    while not time_step.last():\n",
        "      agents_output = [\n",
        "          agent.step(time_step, is_evaluation=True) for agent in agents\n",
        "      ]\n",
        "      action_list = [agent_output.action for agent_output in agents_output]\n",
        "      time_step = env.step(action_list)\n",
        "      episode_rewards += time_step.rewards\n",
        "    sum_episode_rewards += episode_rewards\n",
        "\n",
        "    if episode_rewards[0] < episode_rewards[1]:\n",
        "      losses += 1\n",
        "    elif episode_rewards[0] > episode_rewards[1]:\n",
        "      wins += 1\n",
        "    else:\n",
        "      draws += 1\n",
        "\n",
        "    if verbose:\n",
        "      print(f\"Finished episode {ep}, \"\n",
        "            + f\"avg returns: {sum_episode_rewards / (ep+1)}\")\n",
        "\n",
        "  return sum_episode_rewards / num_episodes, wins, losses, draws\n",
        "\n",
        "def testAgentAndRandomness(randomnessRate: float, agentId: int, numEpisodes: int) -> str:\n",
        "  myAgent = LstmAgent('model_stateDict.pt', random_chance=randomnessRate)\n",
        "  agents = [\n",
        "      myAgent,\n",
        "      create_roshambo_bot_agent(1, 3, roshambo_bot_names, agentId)\n",
        "  ]\n",
        "  env = rl_environment.Environment(\n",
        "    \"repeated_game(stage_game=matrix_rps(),num_repetitions=\" +\n",
        "    f\"{pyspiel.ROSHAMBO_NUM_THROWS},\" +\n",
        "    f\"recall={RECALL})\",\n",
        "    include_full_state=True)\n",
        "\n",
        "  print(f\"Starting eval run for {randomnessRate :.2f} random chance and against agent {roshambo_id_to_name[agentId]}\")\n",
        "  avg_eval_returns, wins, losses, draws = eval_agents_count_winrate(env, agents, 2, numEpisodes, verbose=False)\n",
        "\n",
        "  resultStr = f\"For {randomnessRate :.2f} random rate and against agent {roshambo_id_to_name[agentId]}:\\n\" \\\n",
        "    f\"Avg return: {avg_eval_returns}\\nWins: {wins}\\nLosses: {losses}\\nDraws: {draws}\\nWin rate: {wins / numEpisodes :.2%}\"\n",
        "  return resultStr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Change the specified path in the next cell if running locally instead of on Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "5afNPmZVgbGW"
      },
      "outputs": [],
      "source": [
        "def testAllInRange(start: int, end: int, numEpisodes: int, filename: str):\n",
        "  \"\"\"Tests agents with ids [start, end)\"\"\"\n",
        "  for agentId in range(start, end):\n",
        "    resString = testAgentAndRandomness(0.8, agentId, numEpisodes)\n",
        "    # Change the below file path if running locally instead of on Colab\n",
        "    with open(f'/content/drive/My Drive/CS486A4/RandomnessEvals/{filename}.txt', 'a') as f:\n",
        "      f.write('\\n' + resString + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Colab allows you to have 3 notebooks open at once. To speed up testing, I created 3 copies of this notebook and had each run tests against a subset of the agents. If you want to run everything in this notebook, replace the specified line below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7SIThYrm3Zp",
        "outputId": "8fbdc152-750c-455b-f751-a591417c5da2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting eval run for 0.80 random chance and against agent actr_lag2_decay\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [13:14<00:00,  1.99s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting eval run for 0.80 random chance and against agent adddriftbot2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [12:35<00:00,  1.89s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting eval run for 0.80 random chance and against agent addshiftbot3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [12:31<00:00,  1.88s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting eval run for 0.80 random chance and against agent antiflatbot\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [12:40<00:00,  1.90s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting eval run for 0.80 random chance and against agent antirotnbot\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [12:33<00:00,  1.88s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting eval run for 0.80 random chance and against agent biopic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [12:42<00:00,  1.91s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting eval run for 0.80 random chance and against agent boom\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [12:54<00:00,  1.94s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting eval run for 0.80 random chance and against agent copybot\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [12:36<00:00,  1.89s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting eval run for 0.80 random chance and against agent debruijn81\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [13:03<00:00,  1.96s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting eval run for 0.80 random chance and against agent driftbot\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [13:13<00:00,  1.98s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting eval run for 0.80 random chance and against agent flatbot3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [13:16<00:00,  1.99s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting eval run for 0.80 random chance and against agent foxtrotbot\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [12:42<00:00,  1.91s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting eval run for 0.80 random chance and against agent freqbot2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [12:26<00:00,  1.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting eval run for 0.80 random chance and against agent granite\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [12:29<00:00,  1.87s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting eval run for 0.80 random chance and against agent greenberg\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 400/400 [13:19<00:00,  2.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total runtime: 11540.165916919708\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from time import time\n",
        "\n",
        "start = time()\n",
        "testAllInRange(0, 15, 400, 'finalTest1')  # To split up the work over 3 notebooks\n",
        "# testAllInRange(0, 44, 400, 'finalTest')  # To run everything in this notebook\n",
        "print(\"Total runtime:\", time() - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8qiu5qVm9u9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
